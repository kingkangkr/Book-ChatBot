{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "# 현재 스크립트 위치 기준으로 상대 경로 설정\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"booksummaries.txt\")\n",
    "\n",
    "# 파일 읽기\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# 출력 예시\n",
    "print(data[:1])  # 첫 5줄 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lissa\\desktop\\2025\\헤슬\\책지피티\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lissa\\desktop\\2025\\헤슬\\책지피티\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # lowercasing\n",
    "    lowercased_text = text.lower()\n",
    "\n",
    "    # cleaning \n",
    "    import re \n",
    "    remove_punctuation = re.sub(r'[^\\w\\s]', '', lowercased_text)\n",
    "    remove_white_space = remove_punctuation.strip()\n",
    "\n",
    "    # Tokenization = Breaking down each sentence into an array\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_text = word_tokenize(remove_white_space)\n",
    "\n",
    "    # Stop Words/filtering = Removing irrelevant words\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    stopwords_removed = [word for word in tokenized_text if word not in stopwords]\n",
    "\n",
    "    # Stemming = Transforming words into their base form\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = [ps.stem(word) for word in stopwords_removed]\n",
    "    \n",
    "    # Putting all the results into a dataframe.\n",
    "    df = pd.DataFrame({\n",
    "        'DOCUMENT': [text],\n",
    "        'LOWERCASE' : [lowercased_text],\n",
    "        'CLEANING': [remove_white_space],\n",
    "        'TOKENIZATION': [tokenized_text],\n",
    "        'STOP-WORDS': [stopwords_removed],\n",
    "        'STEMMING': [stemmed_text]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus):\n",
    "    # Call the preprocessing result\n",
    "    df = preprocessing(corpus)\n",
    "        \n",
    "    # Make each array row from stopwords_removed to be a sentence\n",
    "    stemming = corpus['STEMMING'].apply(' '.join)\n",
    "    \n",
    "    # Count TF-IDF\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(stemming)\n",
    "    \n",
    "    # Get words from stopwords array to use as headers\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Combine header titles and weights\n",
    "    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    df_tfidf = pd.concat([df, df_tfidf], axis=1)\n",
    "\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(corpus):\n",
    "    # Call the TF-IDF result\n",
    "    df_tfidf = calculate_tfidf(corpus)\n",
    "    \n",
    "    # Get the TF-IDF vector for the first item (index 0)\n",
    "    vector1 = df_tfidf.iloc[0, 6:].values.reshape(1, -1)\n",
    "\n",
    "    # Get the TF-IDF vector for all items except the first item\n",
    "    vectors = df_tfidf.iloc[:, 6:].values\n",
    "    \n",
    "    # Calculate cosine similarity between the first item and all other items\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    cosim = cosine_similarity(vector1, vectors)\n",
    "    cosim = pd.DataFrame(cosim)\n",
    "    \n",
    "    # Convert the DataFrame into a one-dimensional array\n",
    "    cosim = cosim.values.flatten()\n",
    "\n",
    "    # Convert the cosine similarity result into a DataFrame\n",
    "    df_cosim = pd.DataFrame(cosim, columns=['COSIM'])\n",
    "\n",
    "    # Combine the TF-IDF array with the cosine similarity result\n",
    "    df_cosim = pd.concat([df_tfidf, df_cosim], axis=1)\n",
    "\n",
    "    return df_cosim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일이 저장되었습니다: c:\\Users\\lissa\\Desktop\\2025\\헤슬\\책지피티\\output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the column names\n",
    "column_names = [\n",
    "    'Wikipedia article ID', \n",
    "    'Freebase ID', \n",
    "    'Book title', \n",
    "    'Author', \n",
    "    'Publication date', \n",
    "    'Book genres (Freebase ID:name tuples)', \n",
    "    'Plot summary'\n",
    "]\n",
    "\n",
    "# 현재 작업 디렉토리에서 파일 경로 설정\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"booksummaries.txt\")\n",
    "\n",
    "# 파일이 존재하는지 확인 후 읽기\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path, delimiter='\\t', names=column_names, header=None)  # Adjust delimiter as needed\n",
    "\n",
    "    # Save the DataFrame to a .csv file with column names\n",
    "    output_path = os.path.join(os.getcwd(), \"output.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"CSV 파일이 저장되었습니다: {output_path}\")\n",
    "else:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='./output.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\lissa\\desktop\\2025\\헤슬\\책지피티\\.venv\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lissa\\desktop\\2025\\헤슬\\책지피티\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 7.1/11.1 MB 39.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 31.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.0-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 8.9/43.6 MB 42.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 18.1/43.6 MB 42.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 23.9/43.6 MB 37.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.0/43.6 MB 38.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.0/43.6 MB 40.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 35.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.0 scipy-1.15.0 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16559, 121260)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply TF-IDF on the 'Plot summary' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Plot summary'])\n",
    "\n",
    "# Show the shape of the resulting TF-IDF matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.1299163  0.18956561 ... 0.14520625 0.03338112 0.11606397]\n",
      " [0.1299163  1.         0.17445624 ... 0.13893065 0.02769185 0.11881149]\n",
      " [0.18956561 0.17445624 1.         ... 0.19229554 0.0465808  0.16297015]\n",
      " ...\n",
      " [0.14520625 0.13893065 0.19229554 ... 1.         0.03152995 0.13596954]\n",
      " [0.03338112 0.02769185 0.0465808  ... 0.03152995 1.         0.02981897]\n",
      " [0.11606397 0.11881149 0.16297015 ... 0.13596954 0.02981897 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Show the resulting similarity matrix\n",
    "print(cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(book_title, df, cosine_sim_matrix, top_n=5):\n",
    "    # Reset the index of the DataFrame and create a reverse mapping of indices and book titles\n",
    "    df = df.reset_index()\n",
    "    indices = pd.Series(df.index, index=df['Book title']).drop_duplicates()\n",
    "    \n",
    "    # Check if the book exists in the dataset\n",
    "    if book_title not in indices:\n",
    "        print(f\"Book titled '{book_title}' not found in the dataset.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the index of the book that matches the title\n",
    "    idx = indices[book_title]\n",
    "    \n",
    "    # Get the pairwise similarity scores of all books with that book\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "    \n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Skip the first book (itself) and get the scores of the top_n most similar books\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    \n",
    "    # Get the book indices\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the top_n most similar books\n",
    "    return df['Book title'].iloc[book_indices].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books similar to 'Book of Joshua':\n",
      "1. Book of Numbers\n",
      "2. Veracity\n",
      "3. Kaleidoscope Century\n",
      "4. Joshua Then and Now\n",
      "5. Fevre Dream\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "book_to_recommend = \"Book of Joshua\"\n",
    "recommended_books = recommend_books(book_to_recommend, df, cosine_sim_matrix, top_n=5)\n",
    "\n",
    "print(f\"Books similar to '{book_to_recommend}':\")\n",
    "for idx, title in enumerate(recommended_books, 1):\n",
    "    print(f\"{idx}. {title}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
